{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## \n",
    "##############################",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import glob\n",
    "import librosa\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from util import *\n",
    "from covidxdataset import COVIDxDataset\n",
    "from audiodataset import CoswaraDataset\n",
    "from Image_Audio_dataset import Image_Audio_Dataset\n",
    "import util as util\n",
    "from util import Mel2Samp\n",
    "from Image_Audio_train import train, validation\n",
    "from model import transfer_resNet, ResNet54, ResNet22, ResNet38, MFCC_MEL_Ensemble, Image_Audio_Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../final/Dataset/Coswara-Data/*/*/'\n",
    "pid_list = glob.glob(data_path)\n",
    "\n",
    "paths = []\n",
    "labels = []\n",
    "for pid in pid_list:\n",
    "    json_file = pid + 'metadata.json'\n",
    "    with open(json_file) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        status = json_data[\"covid_status\"]\n",
    "    if 'positive' in status:\n",
    "        status = 'positive'\n",
    "    if status != 'healthy' and status != 'positive':\n",
    "        continue\n",
    "    file_list = glob.glob(pid + '*.wav')\n",
    "    for f in file_list:\n",
    "        if 'cough' not in f:\n",
    "            continue\n",
    "        paths.append(f)\n",
    "        labels.append(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'read_all_filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-48f8bbf52a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'audio'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage_Audio_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage_Audio_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage_Audio_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebook/home/u00u664m0mjzfAyslY357/final/Covid_Image_Audio/Image_Audio_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, n_classes, Image_path, Audio_path, segment_length, dim)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m#'NON-COVID-19': 0, 'COVID-19': 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mImage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIamge_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_all_filepaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage_trainfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mImage_testfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;31m#         Iamge_n_sample = np.sum(Image_labels == 'COVID-19')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m#         Iamege_h_paths = Image_paths[Iamge_labels == 'NON-COVID-19']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_all_filepaths' is not defined"
     ]
    }
   ],
   "source": [
    "seed = 20\n",
    "seed_everything(seed)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "dataset_name = 'audio'\n",
    "if dataset_name == 'image':\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "elif dataset_name == 'audio':\n",
    "    model = MFCC_MEL_Ensemble(classes_num=2, extractor=True)\n",
    "elif dataset_name == 'image_audio':\n",
    "    model = Image_Audio_Ensemble(classes_num=2, extractor=True)\n",
    "    \n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 32\n",
    "train_params = {'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 10}\n",
    "\n",
    "test_params = {'batch_size': batch_size,\n",
    "               'shuffle': False,\n",
    "               'num_workers': 7}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "if dataset_name == 'audio':\n",
    "    train_dataset = Image_Audio_Dataset(mode='train', n_classes=num_classes, segment_length=16000*4)\n",
    "    val_dataset = Image_Audio_Dataset(mode='valid', n_classes=num_classes, segment_length=16000*4)\n",
    "    test_dataset = Image_Audio_Dataset(mode='test', n_classes=num_classes, segment_length=16000*4)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, **train_params)\n",
    "    val_loader = DataLoader(val_dataset, **test_params)\n",
    "    test_loader = DataLoader(test_dataset, **test_params)\n",
    "\n",
    "elif dataset_name == 'image':\n",
    "    train_dataset = Image_Audio_Dataset(mode='train', n_classes=num_classes, dim=(224, 224))\n",
    "    val_dataset = Image_Audio_Dataset(mode='valid', n_classes=num_classes, dim=(224, 224))\n",
    "    test_dataset = Image_Audio_Dataset(mode='test', n_classes=num_classes, dim=(224, 224))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, **train_params)\n",
    "    val_loader = DataLoader(val_dataset, **test_params)\n",
    "    test_loader = DataLoader(test_dataset, **test_params)\n",
    "\n",
    "elif dataset_name == 'image_audio':\n",
    "    train_dataset = Image_Audio_Dataset(mode='train', n_classes=num_classes, segment_length=16000*4, dim=(224, 224))\n",
    "    val_dataset = Image_Audio_Dataset(mode='valid', n_classes=num_classes, segment_length=16000*4, dim=(224, 224))\n",
    "    test_dataset = Image_Audio_Dataset(mode='test', n_classes=num_classes, segment_length=16000*4, dim=(224, 224))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, **train_params)\n",
    "    val_loader = DataLoader(val_dataset, **test_params)\n",
    "    test_loader = DataLoader(test_dataset, **test_params)\n",
    "\n",
    "# print(model)\n",
    "num_epochs = 30\n",
    "best_pred_loss = 1000.0\n",
    "lr_sch = ReduceLROnPlateau(optimizer, factor=0.5, patience=2, min_lr=1e-7, verbose=True)\n",
    "# lr_sch = ExponentialLR(optimizer, gamma=0.975)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(dataset_name,device, batch_size, model, train_loader, optimizer, epoch, None)\n",
    "    val_metrics, confusion_matrix = validation(dataset_name,device, batch_size, num_classes, model, test_loader, epoch, None)\n",
    "\n",
    "#     best_pred_loss = util.save_model(model, optimizer, args, val_metrics, epoch, best_pred_loss, confusion_matrix)\n",
    "    \n",
    "    lr_sch.step(val_metrics.avg('loss'))\n",
    "#     lr_sch.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  3 12:18:28 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           On   | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 41%   34C    P8     1W / 280W |  18901MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX           On   | 00000000:60:00.0 Off |                  N/A |\n",
      "|  0%   38C    P8     7W / 280W |  15831MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN RTX           On   | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 41%   32C    P8    12W / 280W |      0MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN RTX           On   | 00000000:DB:00.0 Off |                  N/A |\n",
      "| 41%   32C    P8    15W / 280W |      0MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Epoch: 1\tSample:    1/  259\tLoss:0.8575\tAccuracy:0.31\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH: 1\tSample:  259/  259\tLoss:2.2181\tAccuracy:0.53\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[53. 77.]\n",
    " [45. 84.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH: 1\tSample:   65/   65\tLoss:3.3669\tAccuracy:0.49\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[32.  0.]\n",
    " [33.  0.]]\n",
    "====================================\n",
    "Epoch: 2\tSample:    1/  259\tLoss:1.7873\tAccuracy:0.47\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH: 2\tSample:  259/  259\tLoss:1.3106\tAccuracy:0.58\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[84. 46.]\n",
    " [64. 65.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH: 2\tSample:   65/   65\tLoss:1.0213\tAccuracy:0.69\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[31.  1.]\n",
    " [19. 14.]]\n",
    "====================================\n",
    "Epoch: 3\tSample:    1/  259\tLoss:0.2789\tAccuracy:0.81\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH: 3\tSample:  259/  259\tLoss:0.3635\tAccuracy:0.83\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[105.  25.]\n",
    " [ 18. 111.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH: 3\tSample:   65/   65\tLoss:0.4288\tAccuracy:0.75\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[16. 16.]\n",
    " [ 0. 33.]]\n",
    "====================================\n",
    "Epoch: 4\tSample:    1/  259\tLoss:0.5580\tAccuracy:0.84\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH: 4\tSample:  259/  259\tLoss:0.5320\tAccuracy:0.76\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[111.  19.]\n",
    " [ 42.  87.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH: 4\tSample:   65/   65\tLoss:2.2815\tAccuracy:0.80\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [11. 22.]]\n",
    "====================================\n",
    "Epoch: 5\tSample:    1/  259\tLoss:0.2437\tAccuracy:0.91\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH: 5\tSample:  259/  259\tLoss:0.4508\tAccuracy:0.87\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[114.  16.]\n",
    " [ 17. 112.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH: 5\tSample:   65/   65\tLoss:0.3597\tAccuracy:0.86\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [ 7. 26.]]\n",
    "====================================\n",
    "Epoch: 6\tSample:    1/  259\tLoss:0.4641\tAccuracy:0.84\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH: 6\tSample:  259/  259\tLoss:0.6279\tAccuracy:0.82\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[ 94.  36.]\n",
    " [ 10. 119.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH: 6\tSample:   65/   65\tLoss:0.2882\tAccuracy:0.85\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[32.  0.]\n",
    " [10. 23.]]\n",
    "====================================\n",
    "Epoch: 7\tSample:    1/  259\tLoss:0.4799\tAccuracy:0.88\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH: 7\tSample:  259/  259\tLoss:0.2702\tAccuracy:0.89\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[114.  16.]\n",
    " [ 12. 117.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH: 7\tSample:   65/   65\tLoss:1.5296\tAccuracy:0.75\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[31.  1.]\n",
    " [15. 18.]]\n",
    "====================================\n",
    "Epoch: 8\tSample:    1/  259\tLoss:0.2861\tAccuracy:0.94\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH: 8\tSample:  259/  259\tLoss:0.5482\tAccuracy:0.85\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[120.  10.]\n",
    " [ 28. 101.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH: 8\tSample:   65/   65\tLoss:0.2510\tAccuracy:0.89\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[25.  7.]\n",
    " [ 0. 33.]]\n",
    "====================================\n",
    "Epoch: 9\tSample:    1/  259\tLoss:0.0850\tAccuracy:0.94\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH: 9\tSample:  259/  259\tLoss:0.4768\tAccuracy:0.88\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[103.  27.]\n",
    " [  4. 125.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH: 9\tSample:   65/   65\tLoss:0.2462\tAccuracy:0.86\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[27.  5.]\n",
    " [ 4. 29.]]\n",
    "====================================\n",
    "Epoch:10\tSample:    1/  259\tLoss:0.2605\tAccuracy:0.91\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:10\tSample:  259/  259\tLoss:0.6331\tAccuracy:0.91\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[115.  15.]\n",
    " [  9. 120.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:10\tSample:   65/   65\tLoss:0.4729\tAccuracy:0.82\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[22. 10.]\n",
    " [ 2. 31.]]\n",
    "====================================\n",
    "Epoch:11\tSample:    1/  259\tLoss:0.2290\tAccuracy:0.91\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:11\tSample:  259/  259\tLoss:0.3118\tAccuracy:0.90\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[116.  14.]\n",
    " [ 12. 117.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:11\tSample:   65/   65\tLoss:0.2774\tAccuracy:0.86\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[24.  8.]\n",
    " [ 1. 32.]]\n",
    "====================================\n",
    "Epoch:12\tSample:    1/  259\tLoss:0.0502\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:12\tSample:  259/  259\tLoss:0.2496\tAccuracy:0.90\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[111.  19.]\n",
    " [  6. 123.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:12\tSample:   65/   65\tLoss:0.4900\tAccuracy:0.85\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[26.  6.]\n",
    " [ 4. 29.]]\n",
    "====================================\n",
    "Epoch    12: reducing learning rate of group 0 to 5.0000e-05.\n",
    "Epoch:13\tSample:    1/  259\tLoss:0.1378\tAccuracy:0.97\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:13\tSample:  259/  259\tLoss:0.1113\tAccuracy:0.95\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[121.   9.]\n",
    " [  3. 126.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:13\tSample:   65/   65\tLoss:0.3729\tAccuracy:0.86\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[28.  4.]\n",
    " [ 5. 28.]]\n",
    "====================================\n",
    "Epoch:14\tSample:    1/  259\tLoss:0.0204\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:14\tSample:  259/  259\tLoss:0.1054\tAccuracy:0.96\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[125.   5.]\n",
    " [  6. 123.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:14\tSample:   65/   65\tLoss:0.5044\tAccuracy:0.86\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[29.  3.]\n",
    " [ 6. 27.]]\n",
    "====================================\n",
    "Epoch:15\tSample:    1/  259\tLoss:0.0458\tAccuracy:0.97\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:15\tSample:  259/  259\tLoss:0.2332\tAccuracy:0.95\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[127.   3.]\n",
    " [ 10. 119.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:15\tSample:   65/   65\tLoss:1.5399\tAccuracy:0.82\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[29.  3.]\n",
    " [ 9. 24.]]\n",
    "====================================\n",
    "Epoch    15: reducing learning rate of group 0 to 2.5000e-05.\n",
    "Epoch:16\tSample:    1/  259\tLoss:0.0176\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:16\tSample:  259/  259\tLoss:0.1206\tAccuracy:0.95\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[127.   3.]\n",
    " [  9. 120.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:16\tSample:   65/   65\tLoss:1.6467\tAccuracy:0.78\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [12. 21.]]\n",
    "====================================\n",
    "Epoch:17\tSample:    1/  259\tLoss:0.0699\tAccuracy:0.97\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:17\tSample:  259/  259\tLoss:0.0705\tAccuracy:0.98\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[128.   2.]\n",
    " [  4. 125.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:17\tSample:   65/   65\tLoss:0.7997\tAccuracy:0.78\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [12. 21.]]\n",
    "====================================\n",
    "Epoch:18\tSample:    1/  259\tLoss:0.0089\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:18\tSample:  259/  259\tLoss:0.2012\tAccuracy:0.97\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[128.   2.]\n",
    " [  5. 124.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:18\tSample:   65/   65\tLoss:1.1171\tAccuracy:0.85\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [ 8. 25.]]\n",
    "====================================\n",
    "Epoch    18: reducing learning rate of group 0 to 1.2500e-05.\n",
    "Epoch:19\tSample:    1/  259\tLoss:0.0552\tAccuracy:0.97\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:19\tSample:  259/  259\tLoss:0.1662\tAccuracy:0.97\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[124.   6.]\n",
    " [  1. 128.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:19\tSample:   65/   65\tLoss:0.6285\tAccuracy:0.86\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [ 7. 26.]]\n",
    "====================================\n",
    "Epoch:20\tSample:    1/  259\tLoss:0.0323\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:20\tSample:  259/  259\tLoss:0.0912\tAccuracy:0.95\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[118.  12.]\n",
    " [  0. 129.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:20\tSample:   65/   65\tLoss:0.8705\tAccuracy:0.86\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[29.  3.]\n",
    " [ 6. 27.]]\n",
    "====================================\n",
    "Epoch:21\tSample:    1/  259\tLoss:0.0329\tAccuracy:0.97\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:21\tSample:  259/  259\tLoss:0.0695\tAccuracy:0.97\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[121.   9.]\n",
    " [  0. 129.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:21\tSample:   65/   65\tLoss:0.2760\tAccuracy:0.86\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[28.  4.]\n",
    " [ 5. 28.]]\n",
    "====================================\n",
    "Epoch    21: reducing learning rate of group 0 to 6.2500e-06.\n",
    "Epoch:22\tSample:    1/  259\tLoss:0.0458\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:22\tSample:  259/  259\tLoss:0.0616\tAccuracy:0.98\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[126.   4.]\n",
    " [  1. 128.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:22\tSample:   65/   65\tLoss:0.5808\tAccuracy:0.86\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [ 7. 26.]]\n",
    "====================================\n",
    "Epoch:23\tSample:    1/  259\tLoss:0.0235\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:23\tSample:  259/  259\tLoss:0.2714\tAccuracy:0.97\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[128.   2.]\n",
    " [  5. 124.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:23\tSample:   65/   65\tLoss:0.9461\tAccuracy:0.85\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[29.  3.]\n",
    " [ 7. 26.]]\n",
    "====================================\n",
    "Epoch:24\tSample:    1/  259\tLoss:0.0428\tAccuracy:0.97\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:24\tSample:  259/  259\tLoss:0.0342\tAccuracy:0.99\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[130.   0.]\n",
    " [  2. 127.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:24\tSample:   65/   65\tLoss:0.6009\tAccuracy:0.83\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [ 9. 24.]]\n",
    "====================================\n",
    "Epoch    24: reducing learning rate of group 0 to 3.1250e-06.\n",
    "Epoch:25\tSample:    1/  259\tLoss:0.0064\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:25\tSample:  259/  259\tLoss:0.1199\tAccuracy:0.98\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[130.   0.]\n",
    " [  4. 125.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:25\tSample:   65/   65\tLoss:1.1645\tAccuracy:0.82\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [10. 23.]]\n",
    "====================================\n",
    "Epoch:26\tSample:    1/  259\tLoss:0.0152\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:26\tSample:  259/  259\tLoss:0.0720\tAccuracy:0.97\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[128.   2.]\n",
    " [  6. 123.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:26\tSample:   65/   65\tLoss:0.9605\tAccuracy:0.82\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [10. 23.]]\n",
    "====================================\n",
    "Epoch:27\tSample:    1/  259\tLoss:0.0306\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:27\tSample:  259/  259\tLoss:0.0219\tAccuracy:1.00\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[130.   0.]\n",
    " [  0. 129.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:27\tSample:   65/   65\tLoss:1.3101\tAccuracy:0.85\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [ 8. 25.]]\n",
    "====================================\n",
    "Epoch    27: reducing learning rate of group 0 to 1.5625e-06.\n",
    "Epoch:28\tSample:    1/  259\tLoss:0.0170\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:28\tSample:  259/  259\tLoss:0.0387\tAccuracy:0.98\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[128.   2.]\n",
    " [  3. 126.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:28\tSample:   65/   65\tLoss:1.1360\tAccuracy:0.83\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [ 9. 24.]]\n",
    "====================================\n",
    "Epoch:29\tSample:    1/  259\tLoss:0.0651\tAccuracy:0.97\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:29\tSample:  259/  259\tLoss:0.0328\tAccuracy:1.00\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[130.   0.]\n",
    " [  1. 128.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:29\tSample:   65/   65\tLoss:0.8152\tAccuracy:0.85\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [ 8. 25.]]\n",
    "====================================\n",
    "Epoch:30\tSample:    1/  259\tLoss:0.0302\tAccuracy:1.00\n",
    "====================================\n",
    "Training\n",
    " SUMMARY EPOCH:30\tSample:  259/  259\tLoss:0.0663\tAccuracy:0.98\n",
    "\n",
    "Training_Confusion Matrix\n",
    "[[127.   3.]\n",
    " [  2. 127.]]\n",
    "====================================\n",
    "====================================\n",
    "Validation\n",
    " SUMMARY EPOCH:30\tSample:   65/   65\tLoss:1.0994\tAccuracy:0.83\n",
    "\n",
    "Validation_Confusion Matrix\n",
    "[[30.  2.]\n",
    " [ 9. 24.]]\n",
    "====================================\n",
    "Epoch    30: reducing learning rate of group 0 to 7.8125e-07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3,224,224).cuda()\n",
    "torch.onnx.export(model, dummy_input, \"MFCC_MEL_Ensemble.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "def build_engine(model_file, max_ws=3*224*224, fp16=False):\n",
    "    print(\"building engine\")\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "    builder.fp16_mode = fp16\n",
    "    config = builder.create_builder_config()\n",
    "    config.max_workspace_size = max_ws\n",
    "    if fp16:\n",
    "        config.flags != 1 << int(trt.BuilderFlag.FP16)\n",
    "    \n",
    "    explicit_batch = 1 << (int) (trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    network = builder.create_network(explicit_batch)\n",
    "    with trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        with open(model_file, 'rb') as model:\n",
    "            parsed = parser.parse(model.read())\n",
    "            print(\"network.num_layers\", network.num_layers)\n",
    "            #last_layer = network.get_layer(network.num_layers - 1)\n",
    "            #network.mark_output(last_layer.get_output(0))\n",
    "            engine = builder.build_engine(network, config=config)\n",
    "            return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_engine' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "engine = build_engine(\"MFCC_MEL_Ensemble.onnx\")\n",
    "with open('MFCC_MEL_Ensemble.trt', 'wb') as f:\n",
    "    f.write(bytearray(engine.serialize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"MFCC_MEL_Ensemble.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# mel, 0.0001, res54\n",
    "Epoch:30\tSample:    1/  320\tLoss:0.1517\tAccuracy:1.00\n",
    "Training\n",
    " SUMMARY EPOCH:30\tSample:  289/  289\tLoss:0.2588\tAccuracy:0.92\n",
    "\n",
    "Confusion Matrix\n",
    "[[143.   9.]\n",
    " [ 16. 136.]]\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "Validation\n",
    " SUMMARY EPOCH:30\tSample:   65/   65\tLoss:0.7340\tAccuracy:0.63\n",
    "\n",
    "Confusion Matrix\n",
    "[[27. 11.]\n",
    " [16. 22.]]\n",
    "# MFCC, 0.0001, res54\n",
    "Epoch:30\tSample:    1/  320\tLoss:0.2430\tAccuracy:0.91\n",
    "Training\n",
    " SUMMARY EPOCH:30\tSample:  289/  289\tLoss:0.2133\tAccuracy:0.92\n",
    "\n",
    "Confusion Matrix\n",
    "[[142.  10.]\n",
    " [ 17. 135.]]\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "Validation\n",
    " SUMMARY EPOCH:30\tSample:   65/   65\tLoss:1.0175\tAccuracy:0.60\n",
    "\n",
    "Confusion Matrix\n",
    "[[13. 25.]\n",
    " [ 3. 35.]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# mel, 0.0001, res22, pretrain\n",
    "Epoch:23\tSample:    1/  320\tLoss:0.1552\tAccuracy:0.97\n",
    "Training\n",
    " SUMMARY EPOCH:23\tSample:  289/  289\tLoss:0.1572\tAccuracy:0.98\n",
    "\n",
    "Confusion Matrix\n",
    "[[148.   4.]\n",
    " [  2. 150.]]\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "Validation\n",
    " SUMMARY EPOCH:23\tSample:   65/   65\tLoss:0.5647\tAccuracy:0.68\n",
    "\n",
    "Confusion Matrix\n",
    "[[29.  9.]\n",
    " [13. 25.]]\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "# mel, 0.0001, res22, aug\n",
    "SUMMARY EPOCH:30\tSample:  289/  289\tLoss:0.3985\tAccuracy:0.81\n",
    "Confusion Matrix\n",
    "[[126.  26.]\n",
    " [ 31. 121.]]\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "Validation\n",
    " SUMMARY EPOCH:30\tSample:   65/   65\tLoss:0.6168\tAccuracy:0.64\n",
    "Confusion Matrix\n",
    "[[24. 14.]\n",
    " [12. 26.]]\n",
    "# mel, 0.0001, res22\n",
    "Training\n",
    " SUMMARY EPOCH:28\tSample:  289/  289\tLoss:0.3153\tAccuracy:0.87\n",
    "\n",
    "Confusion Matrix\n",
    "[[134.  18.]\n",
    " [ 20. 132.]]\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "Validation\n",
    " SUMMARY EPOCH:28\tSample:   65/   65\tLoss:0.6228\tAccuracy:0.74\n",
    "\n",
    "Confusion Matrix\n",
    "[[29.  9.]\n",
    " [11. 27.]]\n",
    "# MFCC, 0.0001, res22\n",
    "Epoch:30\tSample:    1/  320\tLoss:0.3386\tAccuracy:0.84\n",
    "Training\n",
    " SUMMARY EPOCH:30\tSample:  289/  289\tLoss:0.3717\tAccuracy:0.83\n",
    "\n",
    "Confusion Matrix\n",
    "[[125.  27.]\n",
    " [ 23. 129.]]\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "Validation\n",
    " SUMMARY EPOCH:30\tSample:   65/   65\tLoss:0.5852\tAccuracy:0.75\n",
    "\n",
    "Confusion Matrix\n",
    "[[24. 14.]\n",
    " [ 3. 35.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-22153106fc64>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-22153106fc64>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Confusion Matrix\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# mel, 0.0001, shape 다르게\n",
    "Confusion Matrix\n",
    "[[12. 26.]\n",
    " [ 3. 35.]]\n",
    "Epoch    28: reducing learning rate of group 0 to 1.5625e-06.\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "Epoch:29\tSample:    1/  320\tLoss:0.2735\tAccuracy:0.94\n",
    "Training\n",
    " SUMMARY EPOCH:29\tSample:  289/  289\tLoss:0.3475\tAccuracy:0.85\n",
    "\n",
    "Confusion Matrix\n",
    "[[133.  19.]\n",
    " [ 27. 125.]]\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "Validation\n",
    " SUMMARY EPOCH:29\tSample:   65/   65\tLoss:1.0067\tAccuracy:0.60\n",
    "\n",
    "Confusion Matrix\n",
    "[[14. 24.]\n",
    " [ 4. 34.]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# MFCC, 0.0001, shape 다르게\n",
    "Training\n",
    " SUMMARY EPOCH:25\tSample:  289/  289\tLoss:0.2372\tAccuracy:0.92\n",
    "\n",
    "Confusion Matrix\n",
    "[[136.  16.]\n",
    " [ 11. 141.]]\n",
    "('loss', 'correct', 'total', 'accuracy')\n",
    "Validation\n",
    " SUMMARY EPOCH:25\tSample:   65/   65\tLoss:0.6516\tAccuracy:0.75\n",
    "\n",
    "Confusion Matrix\n",
    "[[32.  6.]\n",
    " [15. 23.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics, confusion_matrix = validation(device, batch_size, num_classes, model, test_loader, epoch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load TensorRT File & TensorRT Inference\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "device = cuda.Device(0)\n",
    "context = device.make_context()\n",
    "\n",
    "tensorrt_file_name = 'MMNet.trt'\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    " \n",
    "with open(tensorrt_file_name, 'rb') as f:\n",
    "    engine_data = f.read()\n",
    "engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    " \n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    " \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
    "def allocate_buffers(engine):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "#     ctx.pop()\n",
    "#     del ctx\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is generalized for multiple inputs/outputs.\n",
    "# inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # [cuda.memcpy_htod(inp.device, inp.host) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
    "    # context.execute(batch_size=batch_size, bindings=bindings)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # [cuda.memcpy_dtoh(out.host, out.device) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "\n",
    "    return [out.host for out in outputs]\n",
    "\n",
    "trt_outputs = do_inference(\n",
    "                        context=context,\n",
    "                        bindings=bindings,\n",
    "                        inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        stream=stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_time = []\n",
    "for i in range(1000):\n",
    "    tic = time.time()\n",
    "    trt_outputs = do_inference(\n",
    "                        context=context,\n",
    "                        bindings=bindings,\n",
    "                        inputs=inputs,\n",
    "                        outputs=outputs,\n",
    "                        stream=stream)\n",
    "    toc = time.time()\n",
    "    dur = toc - tic\n",
    "    trt_time.append(dur)\n",
    "print(np.mean(trt_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
