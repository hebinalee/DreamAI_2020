{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "## LOAD IMAGE AND AUDIO DATASET\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Image_Audio_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%file Image_Audio_dataset.py\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import numpy as np\n",
    "from util import read_filepaths\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "import json\n",
    "from util import *\n",
    "\n",
    "\n",
    "class Image_Audio_Dataset(Dataset):\n",
    "    def __init__(self, mode, n_classes=2,\n",
    "                 Image_path='../final/Dataset/CovidX_dataset/',\n",
    "                 Audio_path='../final/Dataset/Coswara-Data/*/*/',\n",
    "                segment_length=16000, dim=(224, 224)):\n",
    "        \n",
    "        self.COVIDxDICT = {'NON-COVID-19': 0, 'COVID-19': 1}\n",
    "        self.AudioDICT = {'healthy': 0, 'positive': 1}\n",
    "        \n",
    "        self.Image_path = Image_path\n",
    "        self.Audio_path = Audio_path\n",
    "        \n",
    "        self.CLASSES = n_classes\n",
    "        self.AudioDICT = {'healthy': 0, 'positive': 1}\n",
    "        self.segment_length = segment_length\n",
    "        pid_list = glob.glob(Audio_path)\n",
    "        self.dim = dim\n",
    "        self.COVIDxDICT = {'NON-COVID-19': 0, 'COVID-19': 1}\n",
    "        Image_testfile = '../final/Dataset/CovidX_dataset/test_split.txt'\n",
    "        Image_trainfile = '../final/Dataset/CovidX_dataset/train_split.txt'\n",
    "        \n",
    "        Audio_paths = []\n",
    "        Audio_labels = []\n",
    "        for pid in pid_list:\n",
    "            json_file = pid + 'metadata.json'\n",
    "            with open(json_file) as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "                status = json_data[\"covid_status\"]\n",
    "            if status == 'positive_mild' or status == 'positive_moderate':\n",
    "                status = 'positive'\n",
    "            if status != 'healthy' and status != 'positive':\n",
    "                continue\n",
    "            file_list = glob.glob(pid + '*.wav')\n",
    "            for f in file_list:\n",
    "                if 'cough' not in f:\n",
    "                    continue\n",
    "                Audio_paths.append(f)\n",
    "                Audio_labels.append(status)\n",
    "        Audio_paths = np.array(Audio_paths)\n",
    "        Audio_labels = np.array(Audio_labels)\n",
    "        \n",
    "#         Audio_n_sample = np.sum(Audio_labels == 'positive')\n",
    "#         Audio_h_paths = Audio_paths[Audio_labels == 'healthy']\n",
    "#         Audio_h_labels = Audio_labels[Audio_labels == 'healthy']\n",
    "        \n",
    "#         idx_sample = np.random.choice(len(Audio_h_paths), Audio_n_sample) #class balance\n",
    "#         Audio_new_paths = np.concatenate([Audio_h_paths[idx_sample], Audio_paths[labels == 'positive']])\n",
    "#         Audio_new_labels = np.concatenate([Audio_h_labels[idx_sample], Audio_labels[labels == 'positive']])\n",
    "        \n",
    "        #'NON-COVID-19': 0, 'COVID-19': 1\n",
    "        Image_paths, Iamge_labels = read_all_filepaths(Image_trainfile,Image_testfile)\n",
    "#         Iamge_n_sample = np.sum(Image_labels == 'COVID-19')\n",
    "#         Iamege_h_paths = Image_paths[Iamge_labels == 'NON-COVID-19']\n",
    "#         Iamege_h_labels = Image_labels[Iamge_labels == 'NON-COVID-19']\n",
    "        \n",
    "#         Iamge_new_paths = np.concatenate([Iamege_h_paths[idx_sample], Image_paths[labels == 'COVID-19']])\n",
    "#         Iamge_new_labels = np.concatenate([Iamege_h_labels[idx_sample], Image_labels[labels == 'COVID-19']])\n",
    "        \n",
    "        Audio_h_paths = Audio_paths[Audio_labels == 'healthy']\n",
    "        Audio_h_lables = Audio_labels[Audio_labels == 'healthy']\n",
    "        \n",
    "        Image_h_paths = Image_paths[Image_labels == 'NON-COVID-19']\n",
    "        Image_h_lables = Iamge_labels[Image_labels == 'NON-COVID-19']\n",
    "        \n",
    "        \n",
    "        #####\n",
    "        print(len(Audio_h_paths))#2000\n",
    "        print(len(Image_h_paths))#50\n",
    "        print(len(Image_paths))#15000\n",
    "        print(np.unique(Iamge_labels, return_counts=True))\n",
    "        print(np.sum(Iamge_labels == 'NON-COVID-19'))\n",
    "        #####\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        paired_paths = []\n",
    "        paired_labels = []\n",
    "        for idx in range(len(Audio_h_paths)):\n",
    "            paired_path = {'audio':Audio_h_paths[idx], 'image':Image_h_paths[idx]} \n",
    "            \n",
    "            paired_paths.append(paired_path)\n",
    "            paired_labels.append(0)\n",
    "        \n",
    "        Audio_covid_paths = Audio_paths[Audio_labels == 'positive']\n",
    "        Audio_covid_lables = Audio_paths[Audio_labels == 'positive']\n",
    "        \n",
    "        Image_covid_paths = Image_paths[Iamge_labels == 'COVID-19']\n",
    "        Image_covid_lables = Image_lables[Iamge_labels == 'COVID-19']\n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx in range(len(Audio_h_paths)):\n",
    "            #paired = {'audio':Audio_covid_paths[idx], 'image':Image_covid_paths[idx], 'label':1}\n",
    "            paired_path = {'audio':Audio_covid_paths[idx], 'image':Image_covid_paths[idx]}\n",
    "            \n",
    "            paired_paths.append(paired)\n",
    "            paired_labels.append(1)\n",
    "        \n",
    "        paired_x, paired_x_test, paired_y, paired_y_test = train_test_split(paired_paths, paired_labels, test_size=0.2, shuffle=True, stratify=Audio_new_labels, random_state=10)\n",
    "        \n",
    "        \n",
    "        if (mode == 'train' or mode == 'valid'):\n",
    "            x_train, x_valid, y_train, y_valid = train_test_split(paired_x, paired_y, test_size=0.1, shuffle=True, stratify=y, random_state=10)\n",
    "            \n",
    "            if mode == 'train':\n",
    "                self.audio_path = paired_x['audio']\n",
    "                self.image_path = paired_x['image']\n",
    "                self.lables = paired_y\n",
    "            elif mode == 'valid' : \n",
    "                self.audio_path = x_valid['audio']\n",
    "                self.image_path = x_valid['image']\n",
    "                self.lables = y_valid\n",
    "        elif (mode=='test'):\n",
    "            self.audio_path = paired_x_test['audio']\n",
    "            self.image_path = paired_x_test['image']\n",
    "            self.lables = paired_y_test\n",
    "            _,cnts = np.unique(self.labels, return_counts=True)\n",
    "            print(\"{} examples =  {}\".format(mode, len(self.paths)), cnts)\n",
    "        \n",
    "        self.root = str(self.Image_path) + '/' + mode + '/'\n",
    "        self.mode = mode\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.audio_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        audio = self.load_audio(self.audio_path[index])\n",
    "        audio = torch.from_numpy(audio)\n",
    "    \n",
    "        # Take segment\n",
    "        if audio.size(0) >= self.segment_length:\n",
    "            max_audio_start = audio.size(0) - self.segment_length\n",
    "            audio_start = random.randint(0, max_audio_start)\n",
    "            audio = audio[audio_start:audio_start+self.segment_length]\n",
    "        else:\n",
    "            audio = torch.nn.functional.pad(audio, (0, self.segment_length - audio.size(0)), 'constant').data\n",
    "#         mel = mel_spectrogram(audio)\n",
    "#         mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=512, n_mels=80, hop_length=160)\n",
    "#         mel = np.expand_dims(mel, 0)\n",
    "        audio = audio.unsqueeze(0)\n",
    "        audio_label_tensor = torch.tensor(self.AudioDICT[self.labels[index]], dtype=torch.long)\n",
    "        \n",
    "        image_tensor = self.load_image(self.root + self.image_path[index], self.dim, augmentation=self.mode)\n",
    "        image_label_tensor = torch.tensor(self.COVIDxDICT[self.labels[index]], dtype=torch.long)\n",
    "\n",
    "        return audio, audio_label_tensor, image_tensor, image_label_tensor\n",
    "\n",
    "    def load_audio(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            print(\"AUDIO DOES NOT EXIST {}\".format(path))\n",
    "        audio, sr = librosa.load(path, sr=16000)\n",
    "#         image_tensor = self.transform(image)\n",
    "\n",
    "        return audio\n",
    "    def load_image(self, img_path, dim, augmentation='test'):\n",
    "        if not os.path.exists(img_path):\n",
    "            print(\"IMAGE DOES NOT EXIST {}\".format(img_path))\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = image.resize(dim)\n",
    "\n",
    "\n",
    "        image_tensor = self.transform(image)\n",
    "\n",
    "        return image_tensor "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
